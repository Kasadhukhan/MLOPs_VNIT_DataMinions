{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa95b192-bde3-4a8a-8c47-a8f1e2d0e2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow...\n",
      "MLflow setup complete!\n",
      "1.24.3\n"
     ]
    }
   ],
   "source": [
    "# Importing all the required libraries in the first step:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import joblib\n",
    "\n",
    "# Configure MLflow\n",
    "print(\"Setting up MLflow...\")\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"spam_detection\")\n",
    "print(\"MLflow setup complete!\")\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2dc778-88a9-45bf-a1b4-74679c633c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Original dataset shape: (5574, 5)\n",
      "\n",
      "Checked and handled any NaN values\n",
      "\n",
      "First few examples:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  label\n",
       "0  go jurong point crazy available bugis n great ...      0\n",
       "1                            ok lar joking wif u oni      0\n",
       "2  free entry wkly comp win fa cup final tkts st ...      1\n",
       "3                u dun say early hor u c already say      0\n",
       "4        nah dont think goes usf lives around though      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution (%):\n",
      "label\n",
      "0    86.598493\n",
      "1    13.401507\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Loading the preprocessed data:\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('../data/preprocessed_data.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Handle NaN values\n",
    "df['cleaned_text'] = df['cleaned_text'].fillna('')\n",
    "print(\"\\nChecked and handled any NaN values\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nFirst few examples:\")\n",
    "display(df[['cleaned_text', 'label']].head())\n",
    "\n",
    "# Show class distribution\n",
    "print(\"\\nClass distribution (%):\")\n",
    "print(df['label'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f8f4d32-2b47-47df-94f0-5cc1d2b3b66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating text features...\n",
      "Features created successfully!\n",
      "Training set shape: (4459, 5000)\n",
      "Testing set shape: (1115, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Create features using TF-IDF\n",
    "print(\"Creating text features...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# Transform text to features\n",
    "X = vectorizer.fit_transform(df['cleaned_text'])\n",
    "y = df['label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Features created successfully!\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d5c1e7e-2e40-41b2-96e4-2514d4d840fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Grid Search...\n",
      "\n",
      "Parameters to be tried:\n",
      "n_estimators: [50, 100, 200]\n",
      "max_depth: [10, 20, 30]\n",
      "min_samples_split: [2, 5, 10]\n",
      "min_samples_leaf: [1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Setup grid search\n",
    "print(\"Setting up Grid Search...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nParameters to be tried:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"{param}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5af2cd0-4cbb-470d-abc9-5b7da903ba6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "\n",
      "Best parameters found:\n",
      "max_depth: 30\n",
      "min_samples_leaf: 1\n",
      "min_samples_split: 10\n",
      "n_estimators: 50\n",
      "\n",
      "Best CV score: 0.9590\n",
      "\n",
      "Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       966\n",
      "           1       1.00      0.75      0.86       149\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.88      0.92      1115\n",
      "weighted avg       0.97      0.97      0.96      1115\n",
      "\n",
      "\n",
      "Model saved to: ../models/spam_classifier.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/15 19:59:10 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\Rahul\\AppData\\Local\\Temp\\tmpiuuzl0du\\model\\model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.6.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/15 19:59:10 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run spam_detection_training at: http://127.0.0.1:5000/#/experiments/219111585967021436/runs/4db30027376a4bffa6e27739f71b3dcb\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/219111585967021436\n",
      "\n",
      "Training complete! Check MLflow UI for details\n"
     ]
    }
   ],
   "source": [
    "# Train model with MLflow tracking\n",
    "print(\"Starting model training...\")\n",
    "with mlflow.start_run(run_name=\"spam_detection_training\"):\n",
    "    # Perform grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    print(f\"\\nBest parameters found:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    print(f\"\\nBest CV score: {best_score:.4f}\")\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"best_cv_score\", best_score)\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    metrics = {\n",
    "        \"test_accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"test_precision\": precision_score(y_test, y_pred),\n",
    "        \"test_recall\": recall_score(y_test, y_pred),\n",
    "        \"test_f1\": f1_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Log metrics\n",
    "    for metric_name, value in metrics.items():\n",
    "        mlflow.log_metric(metric_name, value)\n",
    "    \n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    \n",
    "    # Save model and vectorizer\n",
    "    model_data = {\n",
    "        'vectorizer': vectorizer,\n",
    "        'model': best_model,\n",
    "        'best_params': best_params,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    \n",
    "    model_path = '../models/spam_classifier.joblib'\n",
    "    joblib.dump(model_data, model_path)\n",
    "    print(f\"\\nModel saved to: {model_path}\")\n",
    "    \n",
    "    # Log model in MLflow\n",
    "    mlflow.sklearn.log_model(best_model, \"spam_classifier\")\n",
    "\n",
    "print(\"\\nTraining complete! Check MLflow UI for details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001fe7d2-4b48-490b-91d9-63e8c37077a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spam_detection]",
   "language": "python",
   "name": "conda-env-spam_detection-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
